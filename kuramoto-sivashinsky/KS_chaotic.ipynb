{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arch: modified_MLP\n",
      "Alg: temporal reweighting, Random collocation points\n",
      "Final Time: 1\n",
      "tol:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 6032/200000 [04:03<2:10:29, 24.77it/s, l2 error=0.25495735, Loss=7.0754457, loss_ics=3.832621e-05, loss_res=7.5133476, W_min=0.79451305]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 310\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol: \u001b[39m\u001b[38;5;124m'\u001b[39m, tol)\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Store\u001b[39;00m\n\u001b[1;32m    313\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "Cell \u001b[0;32mIn[4], line 218\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, dataset, nIter)\u001b[0m\n\u001b[1;32m    216\u001b[0m batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_data)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount)\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    221\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:122\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    117\u001b[0m OptimizerState \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizerState\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    118\u001b[0m                             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_def\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtree_defs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    119\u001b[0m register_pytree_node(\n\u001b[1;32m    120\u001b[0m     OptimizerState,\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m xs: ((xs\u001b[38;5;241m.\u001b[39mpacked_state,), (xs\u001b[38;5;241m.\u001b[39mtree_def, xs\u001b[38;5;241m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    125\u001b[0m Array \u001b[38;5;241m=\u001b[39m Any\n\u001b[1;32m    126\u001b[0m Params \u001b[38;5;241m=\u001b[39m Any  \u001b[38;5;66;03m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.experimental.jet import jet\n",
    "from jax.nn import relu\n",
    "# from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the neural net\n",
    "def modified_MLP(layers, L=1.0, M_t=1, M_x=1, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  # Define input encoding function\n",
    "  def input_encoding(t, x):\n",
    "      w = 2 * np.pi / L\n",
    "      k_t = np.power(10, np.arange(-M_t//2, M_t//2))\n",
    "      k_x = np.arange(1, M_x + 1)\n",
    "        \n",
    "      out = np.hstack([k_t * t ,\n",
    "                       1, np.cos(k_x * w * x), np.sin(k_x * w * x)])\n",
    "      return out\n",
    "\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "        \n",
    "      t = inputs[0]\n",
    "      x = inputs[1]\n",
    "      inputs = input_encoding(t, x)  \n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = (1 + 0.01) * t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x,), minval=0.0, maxval=2.0*np.pi)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, u_exact, arch, layers, M_t, M_x, state0, t0, t1, n_t, n_x, tol): \n",
    "        \n",
    "        self.u_exact = u_exact\n",
    "        \n",
    "        self.M_t = M_t\n",
    "        self.M_x = M_x\n",
    "\n",
    "        # grid\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        eps = 0.01 * self.t1\n",
    "        self.t_r   = np.linspace(self.t0, self.t1 + eps, n_t)\n",
    "        self.x_r = np.linspace(0, 2.0 * np.pi, n_x)\n",
    "\n",
    "        # IC\n",
    "        t_ic = np.zeros((x_star.shape[0], 1))\n",
    "        x_ic = x_star.reshape(-1, 1)\n",
    "        self.X_ic = np.hstack([t_ic, x_ic])\n",
    "        self.Y_ic = state0\n",
    "    \n",
    "        # Weight matrix\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T \n",
    "        self.tol = tol\n",
    "\n",
    "\n",
    "        d0 = 2 * M_x + M_t + 1\n",
    "        layers = [d0] + layers\n",
    "        self.init, self.apply = modified_MLP(layers, L=2.0*np.pi, M_t=self.M_t, M_x=self.M_x, activation=np.tanh)\n",
    "        params = self.init(rng_key = key)\n",
    "         \n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init,         self.opt_update,         self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3, \n",
    "                                                                      decay_steps=5000, \n",
    "                                                                      decay_rate=0.9))\n",
    "        self.opt_state = self.opt_init(params) \n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "        \n",
    "        \n",
    "        self.u_pred_fn = vmap(vmap(self.neural_net, (None, 0, None)), (None, None, 0))  # consistent with the dataset\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0)), (None, 0, None))\n",
    "\n",
    "        # Logger\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "        self.l2_error_log = []\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "    \n",
    "    def neural_net(self, params, t, x):\n",
    "        z = np.stack([t, x])\n",
    "        outputs = self.apply(params, z)\n",
    "        return outputs[0]\n",
    "\n",
    "    def residual_net(self, params, t, x): \n",
    "        u = self.neural_net(params, t, x)\n",
    "        u_t = grad(self.neural_net, argnums=1)(params, t, x)\n",
    "\n",
    "        u_fn = lambda x: self.neural_net(params, t, x)\n",
    "        _, (u_x, u_xx, u_xxx, u_xxxx) = jet(u_fn, (x, ), [[1.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "        return u_t + 100.0 / 16.0 * u * u_x + 100.0 / 16.0**2 * u_xx + 100.0 / 16.0**4 * u_xxxx\n",
    "    \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, batch, tol):\n",
    "        t_r, x_r = batch\n",
    "        L_0 = 1e4 * self.loss_ics(params)\n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        L_t = np.mean(r_pred**2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0) ))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u_pred = vmap(self.neural_net, (None, 0, 0))(params, self.X_ic[:,0], self.X_ic[:,1])\n",
    "        # Compute loss\n",
    "        loss_ics = np.mean((self.Y_ic.flatten() - u_pred.flatten())**2)\n",
    "        return loss_ics\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass        \n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        # Compute loss\n",
    "        loss_r = np.mean(r_pred**2)\n",
    "        return loss_r  \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, batch, self.tol)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_l2_error(self, params):\n",
    "        u_pred = self.u_pred_fn(params, t_star[:num_step], x_star)\n",
    "        l2_error = np.linalg.norm(u_pred - self.u_exact) / np.linalg.norm(self.u_exact) \n",
    "        return l2_error\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter = 10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            batch= next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "                \n",
    "                \n",
    "                l2_error_value = self.compute_l2_error(params)\n",
    "                loss_value = self.loss(params, batch)\n",
    "\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params, batch)\n",
    "                \n",
    "                _, _, W_value = self.residuals_and_weights(params, batch, self.tol)\n",
    "\n",
    "                self.l2_error_log.append(l2_error_value)\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'l2 error': l2_error_value,\n",
    "                                  'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value, \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min'  : W_value.min()})\n",
    "                \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           \n",
    "    # Evaluates predictions at test points  \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_u(self, params, X_star):\n",
    "        u_pred = vmap(self.u_net, (None, 0, 0))(params, X_star[:,0], X_star[:,1])\n",
    "        return u_pred\n",
    "\n",
    "\n",
    "data = scipy.io.loadmat('ks_chaotic.mat')\n",
    "# Test data\n",
    "usol = data['usol']\n",
    "\n",
    "t_star = data['t'][0]\n",
    "x_star = data['x'][0]\n",
    "TT, XX = np.meshgrid(t_star, x_star)\n",
    "X_star =  np.hstack((TT.flatten()[:, None], XX.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "# Hpyer-parameters\n",
    "key = random.PRNGKey(1234)\n",
    "M_t = 6\n",
    "M_x = 5\n",
    "layers = [128, 128, 128, 128, 128, 128, 128, 128, 1]\n",
    "num_step = 25\n",
    "t0 = 0.0\n",
    "t1 = t_star[num_step]\n",
    "n_t = 32\n",
    "n_x = 256\n",
    "\n",
    "tol = 1.0\n",
    "tol_list = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "time_step = 0\n",
    "\n",
    "state0 = usol[:, time_step:time_step+1]\n",
    "t_star = data['t'][0][:num_step]\n",
    "x_star = data['x'][0]\n",
    "\n",
    "# Create data set\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "\n",
    "# arch = 'MLP'\n",
    "arch = 'modified_MLP'\n",
    "print('Arch:', arch)\n",
    "print('Alg: temporal reweighting, Random collocation points')\n",
    "\n",
    "\n",
    "N = 250 // num_step\n",
    "\n",
    "u_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    u_exact = usol[:, time_step + k * num_step:time_step + (k+1) * num_step] # (512, num_step)\n",
    "    print('Final Time: {}'.format(k + 1))\n",
    "    model = PINN(key, u_exact, arch, layers, M_t, M_x, state0, t0, t1, n_t, n_x, tol)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:    \n",
    "        model.tol = tol\n",
    "        print('tol: ', tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=200000)\n",
    "        \n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state)\n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    u_pred_list.append(u_pred)\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.loss_log, model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "\n",
    "    np.save(arch + '_u_pred_list.npy', u_pred_list)\n",
    "    np.save(arch + '_params_list.npy', params_list)\n",
    "    np.save(arch + '_losses_list.npy', losses_list)\n",
    "\n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "    error = np.linalg.norm(u_preds - usol[:, time_step:time_step + (k+1) * num_step]) / np.linalg.norm(usol[:, time_step:time_step + (k+1) * num_step]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "    \n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = vmap(model.neural_net, (None, None, 0))(params, t1, x_star)\n",
    "    state0 = u0_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
