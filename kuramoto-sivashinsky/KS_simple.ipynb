{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: modified_MLP\n",
      "Final Time: 0.1\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18000/200000 [01:37<16:23, 184.98it/s, Loss=0.011175714, loss_ics=1.0768222e-06, loss_res=0.010114735, W_min=0.9969569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12247/200000 [00:43<11:08, 280.90it/s, Loss=0.006085805, loss_ics=2.2933318e-07, loss_res=0.005861747, W_min=0.9826571]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 307\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mtol)\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Store\u001b[39;00m\n\u001b[1;32m    310\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state) \n",
      "Cell \u001b[0;32mIn[1], line 237\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, dataset, nIter)\u001b[0m\n\u001b[1;32m    235\u001b[0m batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_data)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    240\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:122\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    117\u001b[0m OptimizerState \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizerState\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    118\u001b[0m                             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_def\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtree_defs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    119\u001b[0m register_pytree_node(\n\u001b[1;32m    120\u001b[0m     OptimizerState,\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m xs: ((xs\u001b[38;5;241m.\u001b[39mpacked_state,), (xs\u001b[38;5;241m.\u001b[39mtree_def, xs\u001b[38;5;241m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    125\u001b[0m Array \u001b[38;5;241m=\u001b[39m Any\n\u001b[1;32m    126\u001b[0m Params \u001b[38;5;241m=\u001b[39m Any  \u001b[38;5;66;03m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.jet import jet\n",
    "from jax.nn import relu\n",
    "# from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "\n",
    "# Define MLP\n",
    "def MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  # Define input encoding function\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        k = np.arange(1, M + 1)\n",
    "        out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "        return out\n",
    "   \n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1.0 / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        H = input_encoding(t, x)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define modified MLP\n",
    "def modified_MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  # Define input encoding function\n",
    "  def input_encoding(t, x):\n",
    "      w = 2 * np.pi / L\n",
    "      k = np.arange(1, M + 1)\n",
    "      out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "      return out\n",
    "\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "        \n",
    "      t = inputs[0]\n",
    "      x = inputs[1]\n",
    "      inputs = input_encoding(t, x)  \n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply     \n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x,), minval=-1.0, maxval=1.0)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n",
    "    \n",
    "  \n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, arch, layers, M_x, state0, t0, t1, n_t, n_x, tol=1.0): \n",
    "        \n",
    "        # grid\n",
    "        eps = 0.01 * t1\n",
    "        self.t_r = np.linspace(t0, t1 + eps, n_t)\n",
    "        self.x_r = np.linspace(-1.0, 1.0, n_x)\n",
    "\n",
    "        # IC\n",
    "        t_ic = np.zeros((x_star.shape[0], 1))\n",
    "        x_ic = x_star.reshape(-1, 1)\n",
    "        self.X_ic = np.hstack([t_ic, x_ic])\n",
    "        self.Y_ic = state0\n",
    "    \n",
    "        # Weight matrix and causal parameter\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T \n",
    "        self.tol = tol\n",
    "              \n",
    "        if arch == 'MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "\n",
    "            \n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        lr = optimizers.exponential_decay(1e-3, decay_steps=5000, decay_rate=0.9)\n",
    "        self.opt_init,  self.opt_update, self.get_params = optimizers.adam(lr)\n",
    "        self.opt_state = self.opt_init(params) \n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "        \n",
    "        # Evaluate functions over a grid\n",
    "        self.u_pred_fn = vmap(vmap(self.neural_net, (None, 0, None)), (None, None, 0))  # consistent with the dataset\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0)), (None, 0, None))\n",
    "\n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        \n",
    "        self.itercount = itertools.count()\n",
    "    \n",
    "    \n",
    "    def neural_net(self, params, t, x):\n",
    "        z = np.stack([t, x])\n",
    "        outputs = self.apply(params, z)\n",
    "        return outputs[0]\n",
    "\n",
    "    def residual_net(self, params, t, x): \n",
    "        u = self.neural_net(params, t, x)\n",
    "        u_t = grad(self.neural_net, argnums=1)(params, t, x)\n",
    "        u_fn = lambda x: self.neural_net(params, t, x) # For using Taylor-mode AD\n",
    "        _, (u_x, u_xx, u_xxx, u_xxxx) = jet(u_fn, (x, ), [[1.0, 0.0, 0.0, 0.0]]) #  Taylor-mode AD\n",
    "        return u_t + 5 * u * u_x + 0.5 * u_xx + 0.005 * u_xxxx\n",
    "    \n",
    "    # Compute the temporal weights\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, batch, tol):\n",
    "        t_r, x_r = batch\n",
    "        L_0 = 1e3 * self.loss_ics(params)\n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        L_t = np.mean(r_pred**2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0) ))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    # Initial condition loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u_pred = vmap(self.neural_net, (None, 0, 0))(params, self.X_ic[:,0], self.X_ic[:,1])\n",
    "        # Compute loss\n",
    "        loss_ics = np.mean((self.Y_ic.flatten() - u_pred.flatten())**2)\n",
    "        return loss_ics\n",
    "\n",
    "    # Residual loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass        \n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        # Compute loss\n",
    "        loss_r = np.mean(r_pred**2)\n",
    "        return loss_r  \n",
    "\n",
    "    # Total loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, batch, self.tol)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter = 10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            # Get batch\n",
    "            batch= next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, batch, self.tol)\n",
    "\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value, \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min'  : W_value.min()})\n",
    "                \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           \n",
    "\n",
    "# Load data\n",
    "data = scipy.io.loadmat('ks_simple.mat')\n",
    "# Test data\n",
    "usol = data['usol']\n",
    "\n",
    "\n",
    "# Hpyer-parameters\n",
    "key = random.PRNGKey(1234)\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "t0 = 0.0\n",
    "t1 = 0.1\n",
    "n_t = 32\n",
    "n_x = 64\n",
    "tol_list = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "layers = [256, 256, 256, 1] # using Fourier embedding so it is not 1\n",
    "\n",
    "# Initial state\n",
    "state0 = usol[:, 0:1]\n",
    "dt = 1 / 250\n",
    "idx = int(t1 / dt)\n",
    "t_star = data['t'][0][:idx]\n",
    "x_star = data['x'][0]\n",
    "\n",
    "# Create data set\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "arch = 'modified_MLP'\n",
    "print('arch:', arch)\n",
    "\n",
    "N = 10\n",
    "u_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "\n",
    "# Time marching\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format((k + 1) * t1))\n",
    "    model = PINN(key, arch, layers, M_x, state0, t0, t1, n_t, n_x)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:    \n",
    "        model.tol = tol\n",
    "        print(\"tol:\", model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=200000)\n",
    "        \n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state) \n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    u_pred_list.append(u_pred)\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.loss_log, model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "\n",
    "    np.save('u_pred_list.npy', u_pred_list)\n",
    "    np.save('params_list.npy', params_list)\n",
    "    np.save('losses_list.npy', losses_list)\n",
    "    \n",
    "    # error \n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "    error = np.linalg.norm(u_preds - usol[:, :(k+1) * idx]) / np.linalg.norm(usol[:, :(k+1) * idx]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "    \n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = vmap(model.neural_net, (None, None, 0))(params, t1, x_star)\n",
    "    state0 = u0_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
