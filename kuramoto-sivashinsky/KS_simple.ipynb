{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: modified_MLP_skip_between_hidden\n",
      "Final Time: 0.1\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30200/200000 [01:54<10:43, 263.71it/s, Loss=0.02523484, loss_ics=1.7303156e-06, loss_res=0.023589645, W_min=0.9931163]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20000/200000 [00:41<06:14, 480.44it/s, Loss=0.0036096722, loss_ics=2.3557277e-07, loss_res=0.0033758665, W_min=0.9900146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 149200/200000 [05:10<01:45, 480.72it/s, Loss=0.00032512523, loss_ics=3.284623e-08, loss_res=0.00029229227, W_min=0.99143666]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [06:56<00:00, 480.43it/s, Loss=0.0006629242, loss_ics=3.171182e-08, loss_res=0.000631274, W_min=0.8241459]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9287/200000 [00:19<06:36, 480.49it/s, Loss=0.00027208176, loss_ics=2.4923148e-08, loss_res=0.00024716806, W_min=0.46995768]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 435\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mtol)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# Store\u001b[39;00m\n\u001b[1;32m    438\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state) \n",
      "Cell \u001b[0;32mIn[1], line 365\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, dataset, nIter)\u001b[0m\n\u001b[1;32m    363\u001b[0m batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_data)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount)\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    368\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/example_libraries/optimizers.py:120\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    115\u001b[0m OptimizerState \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizerState\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m                             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_def\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtree_defs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    117\u001b[0m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[1;32m    118\u001b[0m     OptimizerState,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m xs: ((xs\u001b[38;5;241m.\u001b[39mpacked_state,), (xs\u001b[38;5;241m.\u001b[39mtree_def, xs\u001b[38;5;241m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    123\u001b[0m Array \u001b[38;5;241m=\u001b[39m Any\n\u001b[1;32m    124\u001b[0m Params \u001b[38;5;241m=\u001b[39m Any  \u001b[38;5;66;03m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.jet import jet\n",
    "from jax.nn import relu\n",
    "# from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define MLP\n",
    "def MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  # Define input encoding function\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        k = np.arange(1, M + 1)\n",
    "        out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "        return out\n",
    "   \n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1.0 / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        H = input_encoding(t, x)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define modified MLP\n",
    "def modified_MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  # Define input encoding function\n",
    "  def input_encoding(t, x):\n",
    "      w = 2 * np.pi / L\n",
    "      k = np.arange(1, M + 1)\n",
    "      out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "      return out\n",
    "\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "        \n",
    "      t = inputs[0]\n",
    "      x = inputs[1]\n",
    "      inputs = input_encoding(t, x)  \n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply     \n",
    "\n",
    "def modified_MLP_skip_between_hidden(layers, L=1.0, M=1, activation=relu):\n",
    "    \"\"\"\n",
    "    A modified MLP that only applies skip connections between hidden layers of\n",
    "    the same dimension. Assumes layers is something like:\n",
    "        [d_in, H, H, ..., H, d_out]\n",
    "    where all hidden layers have dimension H.\n",
    "    \n",
    "    Args:\n",
    "      layers: list of ints, e.g. [d_in, H, H, H, d_out]\n",
    "      L: float, domain size for the encoding (for x).\n",
    "      M: number of sin/cos modes in the encoding.\n",
    "      activation: the activation function.\n",
    "    \n",
    "    Returns:\n",
    "      init: function that initializes parameters\n",
    "      apply: function that applies the network given parameters + inputs\n",
    "    \"\"\"\n",
    "    # --- Xavier (Glorot) initializer ---\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    # --- Input encoding: [t, 1, cos(k * w * x), sin(k * w * x)] ---\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        ks = np.arange(1, M + 1)\n",
    "        # dimension: 2*M + 2\n",
    "        out = np.hstack([\n",
    "            t, \n",
    "            1.0,\n",
    "            np.cos(ks * w * x), \n",
    "            np.sin(ks * w * x)\n",
    "        ])\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "          - 'U1,b1' and 'U2,b2' for gating vectors U, V (each in R^H).\n",
    "          - The hidden + output layers of the MLP.\n",
    "        \"\"\"\n",
    "        # 1) Gating parameters\n",
    "        # We assume layers[1] is the first hidden dimension H.\n",
    "        # We'll call that 'H' for convenience:\n",
    "        H = layers[1]\n",
    "        \n",
    "        # Random keys for gating\n",
    "        key_U1, key_U2 = random.split(random.PRNGKey(12345), 2)\n",
    "        \n",
    "        U1, bU1 = xavier_init(key_U1, layers[0], H)\n",
    "        U2, bU2 = xavier_init(key_U2, layers[0], H)\n",
    "\n",
    "        # 2) MLP layers\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            W, b = xavier_init(key, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        # We'll consume subkeys for each layer\n",
    "        subkeys = random.split(rng_key, len(layers))\n",
    "        params_mlp = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d_in = layers[i]\n",
    "            d_out = layers[i + 1]\n",
    "            W, b = init_layer(subkeys[i], d_in, d_out)\n",
    "            params_mlp.append((W, b))\n",
    "\n",
    "        return (params_mlp, U1, bU1, U2, bU2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          1) Encode input\n",
    "          2) Compute gating vectors U, V\n",
    "          3) Hidden layers:\n",
    "             - layer 1: just gating\n",
    "             - layer k>1: skip connection with gating\n",
    "          4) Output layer\n",
    "        \"\"\"\n",
    "        params_mlp, U1, bU1, U2, bU2 = params\n",
    "\n",
    "        # Extract t, x\n",
    "        t, x = inputs[0], inputs[1]\n",
    "\n",
    "        # Step 1: input encoding\n",
    "        x_enc = input_encoding(t, x)  # shape = layers[0]\n",
    "\n",
    "        # Step 2: gating vectors (U, V) in R^H\n",
    "        U = activation(np.dot(x_enc, U1) + bU1)   # shape (H,)\n",
    "        V = activation(np.dot(x_enc, U2) + bU2)   # shape (H,)\n",
    "\n",
    "        # Step 3: hidden layers\n",
    "        # params_mlp[:-1] = all hidden layers\n",
    "        # params_mlp[-1] = final (output) layer\n",
    "        # We assume each hidden layer is dimension H => H.\n",
    "\n",
    "        # -- layer 1 (index 0 in params_mlp) --\n",
    "        W1, b1 = params_mlp[0]\n",
    "        # shape of W1 => (d_in, H)\n",
    "        r1 = activation(np.dot(x_enc, W1) + b1)   # shape (H,)\n",
    "        g1 = r1 * U + (1.0 - r1) * V              # shape (H,)\n",
    "        # No skip from x_enc to g1 => dimension mismatch\n",
    "        h = g1  # (H,)\n",
    "\n",
    "        # -- subsequent hidden layers --\n",
    "        for (Wk, bk) in params_mlp[1:-1]:\n",
    "            # shape of Wk => (H, H)\n",
    "            rk = activation(np.dot(h, Wk) + bk)    # shape (H,)\n",
    "            gk = rk * U + (1.0 - rk) * V           # shape (H,)\n",
    "            h = h + gk  # skip connection\n",
    "\n",
    "        # Step 4: output layer\n",
    "        W_out, b_out = params_mlp[-1]  # shape of W_out => (H, d_out)\n",
    "        y = np.dot(h, W_out) + b_out   # shape (d_out,)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x,), minval=-1.0, maxval=1.0)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n",
    "    \n",
    "  \n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, arch, layers, M_x, state0, t0, t1, n_t, n_x, tol=1.0): \n",
    "        \n",
    "        # grid\n",
    "        eps = 0.01 * t1\n",
    "        self.t_r = np.linspace(t0, t1 + eps, n_t)\n",
    "        self.x_r = np.linspace(-1.0, 1.0, n_x)\n",
    "\n",
    "        # IC\n",
    "        t_ic = np.zeros((x_star.shape[0], 1))\n",
    "        x_ic = x_star.reshape(-1, 1)\n",
    "        self.X_ic = np.hstack([t_ic, x_ic])\n",
    "        self.Y_ic = state0\n",
    "    \n",
    "        # Weight matrix and causal parameter\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T \n",
    "        self.tol = tol\n",
    "              \n",
    "        if arch == 'MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP_skip_between_hidden':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP_skip_between_hidden(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "\n",
    "            \n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        lr = optimizers.exponential_decay(1e-3, decay_steps=5000, decay_rate=0.9)\n",
    "        self.opt_init,  self.opt_update, self.get_params = optimizers.adam(lr)\n",
    "        self.opt_state = self.opt_init(params) \n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "        \n",
    "        # Evaluate functions over a grid\n",
    "        self.u_pred_fn = vmap(vmap(self.neural_net, (None, 0, None)), (None, None, 0))  # consistent with the dataset\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0)), (None, 0, None))\n",
    "\n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        \n",
    "        self.itercount = itertools.count()\n",
    "    \n",
    "    \n",
    "    def neural_net(self, params, t, x):\n",
    "        z = np.stack([t, x])\n",
    "        outputs = self.apply(params, z)\n",
    "        return outputs[0]\n",
    "\n",
    "    def residual_net(self, params, t, x): \n",
    "        u = self.neural_net(params, t, x)\n",
    "        u_t = grad(self.neural_net, argnums=1)(params, t, x)\n",
    "        u_fn = lambda x: self.neural_net(params, t, x) # For using Taylor-mode AD\n",
    "        _, (u_x, u_xx, u_xxx, u_xxxx) = jet(u_fn, (x, ), [[1.0, 0.0, 0.0, 0.0]]) #  Taylor-mode AD\n",
    "        return u_t + 5 * u * u_x + 0.5 * u_xx + 0.005 * u_xxxx\n",
    "    \n",
    "    # Compute the temporal weights\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, batch, tol):\n",
    "        t_r, x_r = batch\n",
    "        L_0 = 1e3 * self.loss_ics(params)\n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        L_t = np.mean(r_pred**2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0) ))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    # Initial condition loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u_pred = vmap(self.neural_net, (None, 0, 0))(params, self.X_ic[:,0], self.X_ic[:,1])\n",
    "        # Compute loss\n",
    "        loss_ics = np.mean((self.Y_ic.flatten() - u_pred.flatten())**2)\n",
    "        return loss_ics\n",
    "\n",
    "    # Residual loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass        \n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        # Compute loss\n",
    "        loss_r = np.mean(r_pred**2)\n",
    "        return loss_r  \n",
    "\n",
    "    # Total loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, batch, self.tol)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter = 10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            # Get batch\n",
    "            batch= next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, batch, self.tol)\n",
    "\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value, \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min'  : W_value.min()})\n",
    "                \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           \n",
    "\n",
    "# Load data\n",
    "data = scipy.io.loadmat('ks_simple.mat')\n",
    "# Test data\n",
    "usol = data['usol']\n",
    "\n",
    "\n",
    "# Hpyer-parameters\n",
    "key = random.PRNGKey(1234)\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "t0 = 0.0\n",
    "t1 = 0.1\n",
    "n_t = 32\n",
    "n_x = 64\n",
    "tol_list = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "layers = [128, 128, 128, 128, 128, 128, 1] # using Fourier embedding so it is not 1\n",
    "\n",
    "# Initial state\n",
    "state0 = usol[:, 0:1]\n",
    "dt = 1 / 250\n",
    "idx = int(t1 / dt)\n",
    "t_star = data['t'][0][:idx]\n",
    "x_star = data['x'][0]\n",
    "\n",
    "# Create data set\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "arch = 'modified_MLP_skip_between_hidden'\n",
    "print('arch:', arch)\n",
    "\n",
    "N = 10\n",
    "u_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "\n",
    "# Time marching\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format((k + 1) * t1))\n",
    "    model = PINN(key, arch, layers, M_x, state0, t0, t1, n_t, n_x)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:    \n",
    "        model.tol = tol\n",
    "        print(\"tol:\", model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=200000)\n",
    "        \n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state) \n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    u_pred_list.append(u_pred)\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.loss_log, model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "\n",
    "    # np.save('u_pred_list.npy', u_pred_list)\n",
    "    # np.save('params_list.npy', params_list)\n",
    "    # np.save('losses_list.npy', losses_list)\n",
    "    \n",
    "    # error \n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "    error = np.linalg.norm(u_preds - usol[:, :(k+1) * idx]) / np.linalg.norm(usol[:, :(k+1) * idx]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "    \n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = vmap(model.neural_net, (None, None, 0))(params, t1, x_star)\n",
    "    state0 = u0_pred\n",
    "\n",
    "    t_star = data['t'][0]\n",
    "    x_star = data['x'][0]\n",
    "    TT, XX = np.meshgrid(t_star, x_star)\n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state) \n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "\n",
    "\n",
    "    # np.save('u_pred_list.npy', u_pred_list)\n",
    "    # np.save('params_list.npy', params_list)\n",
    "    # np.save('losses_list.npy', losses_list)\n",
    "\n",
    "    # error \n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    plt.plot(model.loss_ics_log, label='IC Loss')\n",
    "    plt.plot(model.loss_res_log, label='Residual Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    # Get trained network parameters\n",
    "    params = model.get_params(model.opt_state)\n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    error = np.linalg.norm(u_preds - usol[:, :(k+1) * idx]) / np.linalg.norm(usol[:, :(k+1) * idx]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pcolor(TT, XX, usol, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$x$')\n",
    "    plt.title(r'Exact $u(x)$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pcolor(TT, XX, u_pred, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$x$')\n",
    "    plt.title(r'Predicted $u(x)$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.pcolor(TT, XX, np.abs(usol - u_pred), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$x$')\n",
    "    plt.title('Absolute error')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x_star, usol[:,0], color='blue')\n",
    "    plt.plot(x_star, u_pred[:,0], '--', color='red')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u(t, x)$')\n",
    "    plt.title('$t = 0$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x_star, usol[:,25], color='blue')\n",
    "    plt.plot(x_star, u_pred[:,25], '--', color='red')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u(t, x)$')\n",
    "    plt.title('$t = 0.05$')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(x_star, usol[:,-1], color='blue')\n",
    "    plt.plot(x_star, u_pred[:,-1], '--', color='red')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$u(t, x)$')\n",
    "    plt.title('$t = 0.1$')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    res_data = iter(dataset)\n",
    "    batch = next(res_data)\n",
    "    L_0, L_t, W = model.residuals_and_weights(params, batch, model.tol)\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.plot(model.t_r, W)\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$w(t)$')\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.plot(model.t_r, L_t)\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$\\mathcal{L}(t, \\\\theta)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
