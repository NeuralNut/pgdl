{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: modified_MLP_skip_connections\n",
      "Final Time: 0.1\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (12,), (256,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 408\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mtol)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Store\u001b[39;00m\n\u001b[1;32m    411\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state) \n",
      "Cell \u001b[0;32mIn[1], line 338\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, dataset, nIter)\u001b[0m\n\u001b[1;32m    336\u001b[0m batch\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(res_data)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount)\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    341\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 326\u001b[0m, in \u001b[0;36mPINN.step\u001b[0;34m(self, i, opt_state, batch)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, opt_state, batch):\n\u001b[1;32m    325\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(opt_state)\n\u001b[0;32m--> 326\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_update(i, g, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 32 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 317\u001b[0m, in \u001b[0;36mPINN.loss\u001b[0;34m(self, params, batch)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch):\n\u001b[0;32m--> 317\u001b[0m     L_0, L_t, W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresiduals_and_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(W \u001b[38;5;241m*\u001b[39m L_t \u001b[38;5;241m+\u001b[39m L_0)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 289\u001b[0m, in \u001b[0;36mPINN.residuals_and_weights\u001b[0;34m(self, params, batch, tol)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresiduals_and_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, batch, tol):\n\u001b[1;32m    288\u001b[0m     t_r, x_r \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 289\u001b[0m     L_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_ics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     r_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_pred_fn(params, t_r, x_r)\n\u001b[1;32m    291\u001b[0m     L_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(r_pred\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 299\u001b[0m, in \u001b[0;36mPINN.loss_ics\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_ics\u001b[39m(\u001b[38;5;28mself\u001b[39m, params):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Compute forward pass\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     u_pred \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneural_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_ic\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_ic\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     loss_ics \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_ic\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m-\u001b[39m u_pred\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m, in \u001b[0;36mPINN.neural_net\u001b[0;34m(self, params, t, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneural_net\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, t, x):\n\u001b[1;32m    274\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([t, x])\n\u001b[0;32m--> 275\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m, in \u001b[0;36mmodified_MLP_skip_connections.<locals>.apply\u001b[0;34m(params, inputs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     gated_out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m U \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m out) \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# skip/add\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgated_out\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Final layer (linear)\u001b[39;00m\n\u001b[1;32m    184\u001b[0m W_out, b_out \u001b[38;5;241m=\u001b[39m params_mlp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1050\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:179\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1215\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/phd/lib/python3.10/site-packages/jax/_src/lax/lax.py:128\u001b[0m, in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    126\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (12,), (256,)."
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.jet import jet\n",
    "from jax.nn import relu\n",
    "# from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "\n",
    "# Define MLP\n",
    "def MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  # Define input encoding function\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        k = np.arange(1, M + 1)\n",
    "        out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "        return out\n",
    "   \n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1.0 / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        H = input_encoding(t, x)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define modified MLP\n",
    "def modified_MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  # Define input encoding function\n",
    "  def input_encoding(t, x):\n",
    "      w = 2 * np.pi / L\n",
    "      k = np.arange(1, M + 1)\n",
    "      out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "      return out\n",
    "\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "        \n",
    "      t = inputs[0]\n",
    "      x = inputs[1]\n",
    "      inputs = input_encoding(t, x)  \n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply     \n",
    "\n",
    "def modified_MLP_skip_between_hidden(layers, L=1.0, M=1, activation=relu):\n",
    "    \"\"\"\n",
    "    A modified MLP that only applies skip connections between hidden layers of\n",
    "    the same dimension. Assumes layers is something like:\n",
    "        [d_in, H, H, ..., H, d_out]\n",
    "    where all hidden layers have dimension H.\n",
    "    \n",
    "    Args:\n",
    "      layers: list of ints, e.g. [d_in, H, H, H, d_out]\n",
    "      L: float, domain size for the encoding (for x).\n",
    "      M: number of sin/cos modes in the encoding.\n",
    "      activation: the activation function.\n",
    "    \n",
    "    Returns:\n",
    "      init: function that initializes parameters\n",
    "      apply: function that applies the network given parameters + inputs\n",
    "    \"\"\"\n",
    "    # --- Xavier (Glorot) initializer ---\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    # --- Input encoding: [t, 1, cos(k * w * x), sin(k * w * x)] ---\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        ks = np.arange(1, M + 1)\n",
    "        # dimension: 2*M + 2\n",
    "        out = np.hstack([\n",
    "            t, \n",
    "            1.0,\n",
    "            np.cos(ks * w * x), \n",
    "            np.sin(ks * w * x)\n",
    "        ])\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "          - 'U1,b1' and 'U2,b2' for gating vectors U, V (each in R^H).\n",
    "          - The hidden + output layers of the MLP.\n",
    "        \"\"\"\n",
    "        # 1) Gating parameters\n",
    "        # We assume layers[1] is the first hidden dimension H.\n",
    "        # We'll call that 'H' for convenience:\n",
    "        H = layers[1]\n",
    "        \n",
    "        # Random keys for gating\n",
    "        key_U1, key_U2 = random.split(random.PRNGKey(12345), 2)\n",
    "        \n",
    "        U1, bU1 = xavier_init(key_U1, layers[0], H)\n",
    "        U2, bU2 = xavier_init(key_U2, layers[0], H)\n",
    "\n",
    "        # 2) MLP layers\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            W, b = xavier_init(key, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        # We'll consume subkeys for each layer\n",
    "        subkeys = random.split(rng_key, len(layers))\n",
    "        params_mlp = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d_in = layers[i]\n",
    "            d_out = layers[i + 1]\n",
    "            W, b = init_layer(subkeys[i], d_in, d_out)\n",
    "            params_mlp.append((W, b))\n",
    "\n",
    "        return (params_mlp, U1, bU1, U2, bU2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          1) Encode input\n",
    "          2) Compute gating vectors U, V\n",
    "          3) Hidden layers:\n",
    "             - layer 1: just gating\n",
    "             - layer k>1: skip connection with gating\n",
    "          4) Output layer\n",
    "        \"\"\"\n",
    "        params_mlp, U1, bU1, U2, bU2 = params\n",
    "\n",
    "        # Extract t, x\n",
    "        t, x = inputs[0], inputs[1]\n",
    "\n",
    "        # Step 1: input encoding\n",
    "        x_enc = input_encoding(t, x)  # shape = layers[0]\n",
    "\n",
    "        # Step 2: gating vectors (U, V) in R^H\n",
    "        U = activation(np.dot(x_enc, U1) + bU1)   # shape (H,)\n",
    "        V = activation(np.dot(x_enc, U2) + bU2)   # shape (H,)\n",
    "\n",
    "        # Step 3: hidden layers\n",
    "        # params_mlp[:-1] = all hidden layers\n",
    "        # params_mlp[-1] = final (output) layer\n",
    "        # We assume each hidden layer is dimension H => H.\n",
    "\n",
    "        # -- layer 1 (index 0 in params_mlp) --\n",
    "        W1, b1 = params_mlp[0]\n",
    "        # shape of W1 => (d_in, H)\n",
    "        r1 = activation(np.dot(x_enc, W1) + b1)   # shape (H,)\n",
    "        g1 = r1 * U + (1.0 - r1) * V              # shape (H,)\n",
    "        # No skip from x_enc to g1 => dimension mismatch\n",
    "        h = g1  # (H,)\n",
    "\n",
    "        # -- subsequent hidden layers --\n",
    "        for (Wk, bk) in params_mlp[1:-1]:\n",
    "            # shape of Wk => (H, H)\n",
    "            rk = activation(np.dot(h, Wk) + bk)    # shape (H,)\n",
    "            gk = rk * U + (1.0 - rk) * V           # shape (H,)\n",
    "            h = h + gk  # skip connection\n",
    "\n",
    "        # Step 4: output layer\n",
    "        W_out, b_out = params_mlp[-1]  # shape of W_out => (H, d_out)\n",
    "        y = np.dot(h, W_out) + b_out   # shape (d_out,)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x,), minval=-1.0, maxval=1.0)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n",
    "    \n",
    "  \n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, arch, layers, M_x, state0, t0, t1, n_t, n_x, tol=1.0): \n",
    "        \n",
    "        # grid\n",
    "        eps = 0.01 * t1\n",
    "        self.t_r = np.linspace(t0, t1 + eps, n_t)\n",
    "        self.x_r = np.linspace(-1.0, 1.0, n_x)\n",
    "\n",
    "        # IC\n",
    "        t_ic = np.zeros((x_star.shape[0], 1))\n",
    "        x_ic = x_star.reshape(-1, 1)\n",
    "        self.X_ic = np.hstack([t_ic, x_ic])\n",
    "        self.Y_ic = state0\n",
    "    \n",
    "        # Weight matrix and causal parameter\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T \n",
    "        self.tol = tol\n",
    "              \n",
    "        if arch == 'MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP_skip_connections':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP_skip_connections(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "\n",
    "            \n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        lr = optimizers.exponential_decay(1e-3, decay_steps=5000, decay_rate=0.9)\n",
    "        self.opt_init,  self.opt_update, self.get_params = optimizers.adam(lr)\n",
    "        self.opt_state = self.opt_init(params) \n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "        \n",
    "        # Evaluate functions over a grid\n",
    "        self.u_pred_fn = vmap(vmap(self.neural_net, (None, 0, None)), (None, None, 0))  # consistent with the dataset\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0)), (None, 0, None))\n",
    "\n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        \n",
    "        self.itercount = itertools.count()\n",
    "    \n",
    "    \n",
    "    def neural_net(self, params, t, x):\n",
    "        z = np.stack([t, x])\n",
    "        outputs = self.apply(params, z)\n",
    "        return outputs[0]\n",
    "\n",
    "    def residual_net(self, params, t, x): \n",
    "        u = self.neural_net(params, t, x)\n",
    "        u_t = grad(self.neural_net, argnums=1)(params, t, x)\n",
    "        u_fn = lambda x: self.neural_net(params, t, x) # For using Taylor-mode AD\n",
    "        _, (u_x, u_xx, u_xxx, u_xxxx) = jet(u_fn, (x, ), [[1.0, 0.0, 0.0, 0.0]]) #  Taylor-mode AD\n",
    "        return u_t + 5 * u * u_x + 0.5 * u_xx + 0.005 * u_xxxx\n",
    "    \n",
    "    # Compute the temporal weights\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, batch, tol):\n",
    "        t_r, x_r = batch\n",
    "        L_0 = 1e3 * self.loss_ics(params)\n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        L_t = np.mean(r_pred**2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0) ))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    # Initial condition loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u_pred = vmap(self.neural_net, (None, 0, 0))(params, self.X_ic[:,0], self.X_ic[:,1])\n",
    "        # Compute loss\n",
    "        loss_ics = np.mean((self.Y_ic.flatten() - u_pred.flatten())**2)\n",
    "        return loss_ics\n",
    "\n",
    "    # Residual loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass        \n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        # Compute loss\n",
    "        loss_r = np.mean(r_pred**2)\n",
    "        return loss_r  \n",
    "\n",
    "    # Total loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, batch, self.tol)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter = 10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            # Get batch\n",
    "            batch= next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, batch, self.tol)\n",
    "\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value, \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min'  : W_value.min()})\n",
    "                \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           \n",
    "\n",
    "# Load data\n",
    "data = scipy.io.loadmat('ks_simple.mat')\n",
    "# Test data\n",
    "usol = data['usol']\n",
    "\n",
    "\n",
    "# Hpyer-parameters\n",
    "key = random.PRNGKey(1234)\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "t0 = 0.0\n",
    "t1 = 0.1\n",
    "n_t = 32\n",
    "n_x = 64\n",
    "tol_list = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "layers = [256, 256, 256, 1] # using Fourier embedding so it is not 1\n",
    "\n",
    "# Initial state\n",
    "state0 = usol[:, 0:1]\n",
    "dt = 1 / 250\n",
    "idx = int(t1 / dt)\n",
    "t_star = data['t'][0][:idx]\n",
    "x_star = data['x'][0]\n",
    "\n",
    "# Create data set\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "arch = 'modified_MLP_skip_connections'\n",
    "print('arch:', arch)\n",
    "\n",
    "N = 10\n",
    "u_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "\n",
    "# Time marching\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format((k + 1) * t1))\n",
    "    model = PINN(key, arch, layers, M_x, state0, t0, t1, n_t, n_x)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:    \n",
    "        model.tol = tol\n",
    "        print(\"tol:\", model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=200000)\n",
    "        \n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state) \n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    u_pred_list.append(u_pred)\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.loss_log, model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "\n",
    "    # np.save('u_pred_list.npy', u_pred_list)\n",
    "    # np.save('params_list.npy', params_list)\n",
    "    # np.save('losses_list.npy', losses_list)\n",
    "    \n",
    "    # error \n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "    error = np.linalg.norm(u_preds - usol[:, :(k+1) * idx]) / np.linalg.norm(usol[:, :(k+1) * idx]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "    \n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = vmap(model.neural_net, (None, None, 0))(params, t1, x_star)\n",
    "    state0 = u0_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([-1.4112451 , -0.41653627,  0.25195843, ..., -0.00153481,\n",
       "        -0.04036552,  0.04120134], dtype=float32),\n",
       " Array([ 0.17148538,  0.42057383, -0.18817493, ..., -0.02769868,\n",
       "         0.00720599,  0.04698551], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
