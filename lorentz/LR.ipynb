{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Time: 0.5\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 24000/300000 [00:18<03:36, 1275.37it/s, Loss=0.0018012591, loss_ics=0.0, loss_res=0.0018012591, W_min=0.99948794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s, Loss=0.0021032023, loss_ics=0.0, loss_res=0.0021032023, W_min=0.99400306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 43000/300000 [00:27<02:47, 1537.41it/s, Loss=0.00022942836, loss_ics=0.0, loss_res=0.00022942836, W_min=0.9934804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [03:15<00:00, 1537.98it/s, Loss=0.0001142779, loss_ics=0.0, loss_res=0.0001142779, W_min=0.9665146]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [03:13<00:00, 1548.96it/s, Loss=0.000111724476, loss_ics=0.0, loss_res=0.000111724476, W_min=0.7174511] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error x: 5.744e-04\n",
      "Relative l2 error y: 6.242e-04\n",
      "Relative l2 error z: 5.312e-04\n",
      "Final Time: 1.0\n",
      "tol: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 14000/300000 [00:11<03:45, 1265.86it/s, Loss=0.02855781, loss_ics=0.0, loss_res=0.02855781, W_min=0.9914442]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7000/300000 [00:04<03:10, 1534.29it/s, Loss=0.0021632195, loss_ics=0.0, loss_res=0.0021632195, W_min=0.9936363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 58000/300000 [00:38<02:39, 1521.32it/s, Loss=0.00024217251, loss_ics=0.0, loss_res=0.00024217251, W_min=0.99290067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 91000/300000 [00:58<02:15, 1544.39it/s, Loss=2.6414851e-05, loss_ics=0.0, loss_res=2.6414851e-05, W_min=0.99227595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11112/300000 [00:07<03:07, 1542.80it/s, Loss=9.489641e-05, loss_ics=0.0, loss_res=9.489641e-05, W_min=0.7541676] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 297\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtol:\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mtol)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_params(model\u001b[38;5;241m.\u001b[39mopt_state)\n\u001b[1;32m    300\u001b[0m x_pred, y_pred, z_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_u(params, t)\n",
      "Cell \u001b[0;32mIn[2], line 229\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, nIter)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitercount)\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    232\u001b[0m         params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/phd/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:122\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    117\u001b[0m OptimizerState \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizerState\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    118\u001b[0m                             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacked_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_def\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtree_defs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    119\u001b[0m register_pytree_node(\n\u001b[1;32m    120\u001b[0m     OptimizerState,\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m xs: ((xs\u001b[38;5;241m.\u001b[39mpacked_state,), (xs\u001b[38;5;241m.\u001b[39mtree_def, xs\u001b[38;5;241m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    125\u001b[0m Array \u001b[38;5;241m=\u001b[39m Any\n\u001b[1;32m    126\u001b[0m Params \u001b[38;5;241m=\u001b[39m Any  \u001b[38;5;66;03m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.nn import relu\n",
    "# from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.optimize import lsq_linear\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "from scipy.optimize import  least_squares\n",
    "\n",
    "from scipy.integrate import odeint  as scipy_odeint\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the neural net\n",
    "def init_layer(key, d_in, d_out):\n",
    "    k1, k2 = random.split(key)\n",
    "    glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "    W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "    b = np.zeros(d_out)\n",
    "    return W, b\n",
    "\n",
    "def MLP(layers, activation=relu):\n",
    "  ''' Vanilla MLP'''\n",
    "  def init(rng_key):\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "  def apply(params, inputs):\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = np.dot(inputs, W) + b\n",
    "          inputs = activation(outputs)\n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "\n",
    "# Define the neural net\n",
    "def modified_MLP(layers, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply\n",
    "\n",
    "# Define Fourier feature net\n",
    "def MLP_FF(layers, sigma=1.0, activation=relu):\n",
    "   # Define input encoding function\n",
    "    def input_encoding(x, w):\n",
    "        out = np.hstack([np.sin(np.dot(x, w)),\n",
    "                         np.cos(np.dot(x, w))])\n",
    "        return out\n",
    "    freqs = sigma * random.normal(random.PRNGKey(0), (layers[0], layers[1]//2))\n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1.0 / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[1:-1], layers[2:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        H = input_encoding(inputs, freqs)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, layers, states0, t0, t1, tol): \n",
    "\n",
    "        self.states0 = states0\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1 \n",
    "        \n",
    "        # Grid\n",
    "        n_t = 300\n",
    "        eps = 0.1 * self.t1\n",
    "        self.t = np.linspace(self.t0, self.t1 + eps, n_t)   \n",
    "\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T\n",
    "        self.tol = tol  \n",
    "\n",
    "        self.rho = 28.0\n",
    "        self.sigma = 10.0\n",
    "        self.beta = 8.0 / 3.0\n",
    "\n",
    "        self.init, self.apply = MLP(layers, activation=np.tanh)\n",
    "        # self.init, self.apply = modified_MLP(layers, activation=np.tanh)\n",
    "        params = self.init(random.PRNGKey(1234))\n",
    "\n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init, \\\n",
    "        self.opt_update, \\\n",
    "        self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3, \n",
    "                                                                      decay_steps=5000, \n",
    "                                                                      decay_rate=0.9))\n",
    "        self.opt_state = self.opt_init(params)\n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "\n",
    "        # Logger\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "    \n",
    "    def neural_net(self, params, t):\n",
    "        t = np.stack([t])\n",
    "        outputs = self.apply(params, t) *  t\n",
    "        x = outputs[0] + self.states0[0]\n",
    "        y = outputs[1] + self.states0[1]\n",
    "        z = outputs[2] + self.states0[2]\n",
    "        return x, y, z\n",
    "\n",
    "    def x_fn(self, params, t):\n",
    "        x, _, _ = self.neural_net(params, t)\n",
    "        return x\n",
    "\n",
    "    def y_fn(self, params, t):\n",
    "        _, y, _ = self.neural_net(params, t)\n",
    "        return y\n",
    "\n",
    "    def z_fn(self, params, t):\n",
    "        _, _, z = self.neural_net(params, t)\n",
    "        return z\n",
    "\n",
    "    def residual_net(self, params, t): \n",
    "        x, y, z = self.neural_net(params, t)\n",
    "        x_t = grad(self.x_fn, argnums=1)(params, t)\n",
    "        y_t = grad(self.y_fn, argnums=1)(params, t)\n",
    "        z_t = grad(self.z_fn, argnums=1)(params, t)\n",
    "\n",
    "        res_1 = x_t - self.sigma * (y - x)\n",
    "        res_2 = y_t - x * (self.rho - z) + y\n",
    "        res_3 = z_t - x * y + self.beta * z\n",
    "\n",
    "        return res_1, res_2, res_3\n",
    " \n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        x_pred, y_pred, z_pred =self.neural_net(params, self.t0)\n",
    "        # Compute loss\n",
    "\n",
    "        loss_x_ic = np.mean((self.states0[0] - x_pred)**2)\n",
    "        loss_y_ic = np.mean((self.states0[1] - y_pred)**2)\n",
    "        loss_z_ic = np.mean((self.states0[2] - z_pred)**2)\n",
    "        return loss_x_ic + loss_y_ic + loss_z_ic\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, tol):\n",
    "        r1_pred, r2_pred, r3_pred = vmap(self.residual_net, (None, 0))(params, self.t)\n",
    "        W = lax.stop_gradient(np.exp(- tol * self.M @ (r1_pred**2 + r2_pred**2 + r3_pred**2)))\n",
    "        return r1_pred, r2_pred, r3_pred, W\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params):\n",
    "        # Compute forward pass\n",
    "        r1_pred, r2_pred, r3_pred, W = self.residuals_and_weights(params, self.tol)\n",
    "        # Compute loss\n",
    "        loss_res = np.mean(W * (r1_pred**2 +  r2_pred**2 + r3_pred**2))\n",
    "        return loss_res\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params):\n",
    "\n",
    "        loss_res = self.loss_res(params)\n",
    "\n",
    "        loss = loss_res\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, nIter = 10000):\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state)\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                loss_value = self.loss(params)\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params)\n",
    "                _, _, _, W_value = self.residuals_and_weights(params, self.tol)\n",
    "\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value,  \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min': W_value.min()} )\n",
    "                                  \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           \n",
    "    # Evaluates predictions at test points  \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_u(self, params, t_star):\n",
    "        x_pred, y_pred, z_pred = vmap(self.neural_net, (None, 0))(params, t_star)\n",
    "        return x_pred, y_pred, z_pred\n",
    "\n",
    "def f(state, t):\n",
    "    x, y, z = state  # Unpack the state vector\n",
    "    return sigma * (y - x), x * (rho - z) - y, x * y - beta * z  # Derivatives\n",
    "\n",
    "rho = 28.0\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "\n",
    "state0 = [1.0, 1.0, 1.0]\n",
    "\n",
    "T = 30\n",
    "t_star = onp.arange(0, T, 0.01)\n",
    "states = scipy_odeint(f, state0, t_star)\n",
    "\n",
    "# Create PINNs model\n",
    "t0 = 0.0\n",
    "t1 = 0.5\n",
    "tol = 0.1\n",
    "\n",
    "tol_list = [1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "\n",
    "layers = [1, 512, 512, 512, 3]\n",
    "\n",
    "x_pred_list = []\n",
    "y_pred_list = []\n",
    "z_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "state0 = np.array([1.0, 1.0, 1.0])\n",
    "t = np.arange(t0, t1, 0.01)\n",
    "for k in range(int(T / t1)):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format( (k+1) * t1))\n",
    "    model = PINN(layers, state0, t0, t1, tol)\n",
    "    \n",
    "    for tol in tol_list:\n",
    "        model.tol = tol\n",
    "        print('tol:', model.tol)\n",
    "        # Train\n",
    "        model.train(nIter=300000)\n",
    "        \n",
    "    params = model.get_params(model.opt_state)\n",
    "    x_pred, y_pred, z_pred = model.predict_u(params, t)\n",
    "    x0_pred, y0_pred, z0_pred = model.neural_net(params, model.t1)\n",
    "    state0 = np.array([x0_pred, y0_pred, z0_pred])\n",
    "    \n",
    "    # Store predictions\n",
    "    x_pred_list.append(x_pred)\n",
    "    y_pred_list.append(y_pred)\n",
    "    z_pred_list.append(z_pred)\n",
    "    losses_list.append([model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "    # Store params\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    \n",
    "    np.save('x_pred_list.npy', x_pred_list)\n",
    "    np.save('y_pred_list.npy', y_pred_list)\n",
    "    np.save('z_pred_list.npy', z_pred_list)\n",
    "    np.save('params_list.npy', params_list)\n",
    "    np.save('losses_list.npy', losses_list)\n",
    "    \n",
    "    # Error\n",
    "    t_star = onp.arange(t0, (k+1) * t1, 0.01)\n",
    "    states = scipy_odeint(f, [1.0, 1.0, 1.0], t_star)\n",
    "    \n",
    "    x_preds = np.hstack(x_pred_list)\n",
    "    y_preds = np.hstack(y_pred_list)\n",
    "    z_preds = np.hstack(z_pred_list)\n",
    "    \n",
    "    error_x = np.linalg.norm(x_preds - states[:, 0]) / np.linalg.norm(states[:, 0]) \n",
    "    error_y = np.linalg.norm(y_preds - states[:, 1]) / np.linalg.norm(states[:, 1]) \n",
    "    error_z = np.linalg.norm(z_preds - states[:, 2]) / np.linalg.norm(states[:, 2]) \n",
    "    print('Relative l2 error x: {:.3e}'.format(error_x))\n",
    "    print('Relative l2 error y: {:.3e}'.format(error_y))\n",
    "    print('Relative l2 error z: {:.3e}'.format(error_z))\n",
    "\n",
    "    \n",
    "np.save('x_pred_list.npy', x_pred_list)\n",
    "np.save('y_pred_list.npy', y_pred_list)\n",
    "np.save('z_pred_list.npy', z_pred_list)\n",
    "np.save('params_list.npy', params_list)\n",
    "\n",
    "x_preds = np.hstack(x_pred_list)\n",
    "y_preds = np.hstack(y_pred_list)\n",
    "z_preds = np.hstack(z_pred_list)\n",
    "\n",
    "error_x = np.linalg.norm(x_preds - states[:, 0]) / np.linalg.norm(states[:, 0]) \n",
    "error_y = np.linalg.norm(y_preds - states[:, 1]) / np.linalg.norm(states[:, 1]) \n",
    "error_z = np.linalg.norm(z_preds - states[:, 2]) / np.linalg.norm(states[:, 2]) \n",
    "print('Relative l2 error x: {:.3e}'.format(error_x))\n",
    "print('Relative l2 error y: {:.3e}'.format(error_y))\n",
    "print('Relative l2 error z: {:.3e}'.format(error_z))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
