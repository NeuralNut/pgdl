{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from flax.traverse_util import flatten_dict\n",
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "from tqdm import trange\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 0) Visualization Helpers\n",
    "# -------------------------------------------------------------------------\n",
    "def _find_subplot_dims_approx_4_3(n: int):\n",
    "    \"\"\"\n",
    "    Find (rows, cols) such that:\n",
    "      1) rows * cols >= n\n",
    "      2) ratio = cols / rows is as close as possible to 4/3\n",
    "    \"\"\"\n",
    "    best_rc = (1, n)\n",
    "    best_diff = float('inf')\n",
    "    for r in range(1, n+1):\n",
    "        c = math.ceil(n / r)\n",
    "        if r * c >= n:\n",
    "            ratio = c / r\n",
    "            diff = abs(ratio - (4/3))\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_rc = (r, c)\n",
    "    return best_rc\n",
    "\n",
    "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plot histograms for each array in val_dict on a grid whose aspect ratio \n",
    "    is as close to 4:3 as possible.\n",
    "\n",
    "    :param val_dict: dict {name -> np.ndarray}\n",
    "    :param color: plot color\n",
    "    :param xlabel: x-axis label\n",
    "    :param stat: 'count' or 'density'\n",
    "    :param use_kde: whether to overlay a kernel density estimate\n",
    "    \"\"\"\n",
    "    n_plots = len(val_dict)\n",
    "    if n_plots == 0:\n",
    "        print(\"No data to plot.\")\n",
    "        return None\n",
    "\n",
    "    # Figure out a suitable grid arrangement\n",
    "    rows, cols = _find_subplot_dims_approx_4_3(n_plots)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(rows, cols, \n",
    "                             figsize=(3.0 * cols, 2.5 * rows),\n",
    "                             squeeze=False)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Plot each array in val_dict\n",
    "    for i, key in enumerate(val_dict.keys()):\n",
    "        vals = val_dict[key].ravel()  # flatten\n",
    "        if log_scale:\n",
    "            vals = np.log1p(vals)\n",
    "        ax = axes[i]\n",
    "        # Use Seaborn for histogram\n",
    "        sns.histplot(vals, ax=ax, color=color, bins=50, stat=stat,\n",
    "                     kde=(use_kde and np.std(vals) > 1e-9), log_scale=log_scale)\n",
    "        ax.set_title(f\"{key}\")\n",
    "        print(f\"{key}: Mean={np.mean(vals):.3e}, Std={np.std(vals):.3e}\")\n",
    "        if xlabel:\n",
    "            ax.set_xlabel(xlabel)\n",
    "\n",
    "    # If there are unused subplots (e.g. rows*cols > n_plots), hide them\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_wave_pde_points(\n",
    "    res: np.ndarray, \n",
    "    b_left: np.ndarray, \n",
    "    b_right: np.ndarray, \n",
    "    b_lower: np.ndarray, \n",
    "    b_upper: np.ndarray\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots collocation points (res) and boundary points (b_left, b_right, \n",
    "    b_lower, b_upper) for a 1D wave PDE on [0,1]x[0,1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    res : (N,2) ndarray\n",
    "        Interior (collocation) points, each row is (x, t).\n",
    "    b_left : (N,2) ndarray\n",
    "        Points where x=0.\n",
    "    b_right : (N,2) ndarray\n",
    "        Points where x=1.\n",
    "    b_lower : (N,2) ndarray\n",
    "        Points where t=0 (initial condition).\n",
    "    b_upper : (N,2) ndarray\n",
    "        Points where t=1.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    # Collocation (interior) points\n",
    "    ax.scatter(res[:,0], res[:,1], \n",
    "               c='k', alpha=0.3, s=8, label='Collocation (res)')\n",
    "\n",
    "    # Boundary: x=0\n",
    "    ax.scatter(b_left[:,0], b_left[:,1], \n",
    "               c='blue', s=20, label='Left boundary (x=0)')\n",
    "\n",
    "    # Boundary: x=1\n",
    "    ax.scatter(b_right[:,0], b_right[:,1], \n",
    "               c='red', s=20, label='Right boundary (x=1)')\n",
    "\n",
    "    # Initial condition: t=0\n",
    "    ax.scatter(b_lower[:,0], b_lower[:,1], \n",
    "               c='green', s=20, label='Initial condition (t=0)')\n",
    "\n",
    "    # \"Top\" boundary: t=1\n",
    "    ax.scatter(b_upper[:,0], b_upper[:,1], \n",
    "               c='magenta', s=20, label='t=1 boundary')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('t')\n",
    "    ax.set_title('Wave PDE points: collocation & boundaries')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "def flatten_intermediates(intdict):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary of \"intermediates\" returned \n",
    "    by model.apply(..., capture_intermediates=True, mutable=[\"intermediates\"]).\n",
    "    \n",
    "    Returns a dict of {name -> np.ndarray}, \n",
    "    where 'name' is a path of keys joined by '/'.\n",
    "    \"\"\"\n",
    "    all_acts = {}\n",
    "\n",
    "    def recurse(prefix, d):\n",
    "        if isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                recurse(prefix + [k], v)\n",
    "        elif isinstance(d, (list, tuple)):\n",
    "            for i, v2 in enumerate(d):\n",
    "                recurse(prefix + [str(i)], v2)\n",
    "        elif isinstance(d, jnp.ndarray):\n",
    "            # Convert to numpy for plotting\n",
    "            name = \"/\".join(prefix)\n",
    "            all_acts[name] = np.array(d)\n",
    "        else:\n",
    "            # ignore non-array\n",
    "            pass\n",
    "\n",
    "    recurse([], intdict)\n",
    "    return all_acts\n",
    "\n",
    "\n",
    "def plot_losses(pinn):\n",
    "    \"\"\"\n",
    "    Plot the training losses (total, residual, IC, BC) \n",
    "    logged in the MambaWavePINN instance.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(pinn.loss_log,     label=\"Total loss\")\n",
    "    plt.plot(pinn.loss_res_log, label=\"Residual loss\")\n",
    "    plt.plot(pinn.loss_ic_log,  label=\"IC loss\")\n",
    "    plt.plot(pinn.loss_bc_log,  label=\"BC loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses per iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weight_distributions(pinn):\n",
    "    \"\"\"\n",
    "    Flatten only the 'weight' parameters in pinn.params (where p.ndim>1)\n",
    "    and plot their distributions with 'plot_dists' (already defined).\n",
    "    \"\"\"\n",
    "    flat_params = tree_flatten(pinn.params)[0]  # list of leaves\n",
    "    weight_vals = {}\n",
    "    w_idx = 0\n",
    "    for p in flat_params:\n",
    "        if p.ndim > 1:  # treat anything with >1D as a 'weight'\n",
    "            weight_vals[f\"W_{w_idx}\"] = np.array(p).ravel()\n",
    "            w_idx += 1\n",
    "\n",
    "    fig = plot_dists(weight_vals, color=\"C0\", xlabel=\"Weight values\")\n",
    "    fig.suptitle(\"Weight Distribution\", fontsize=14, y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gradient_distributions(pinn):\n",
    "    \"\"\"\n",
    "    Compute PDE gradients w.r.t. pinn.params, flatten only 'weight' grads,\n",
    "    and plot using 'plot_dists'.\n",
    "    \"\"\"\n",
    "    grads = pinn.get_gradients(pinn.params)     # PyTree of gradients\n",
    "    leaves = tree_flatten(grads)[0]             # list of leaves\n",
    "    grad_vals = {}\n",
    "    g_idx = 0\n",
    "    for g in leaves:\n",
    "        if g.ndim > 1:  # treat anything with >1D as 'weight grads'\n",
    "            grad_vals[f\"G_{g_idx}\"] = np.array(g).ravel()\n",
    "            g_idx += 1\n",
    "\n",
    "    fig = plot_dists(grad_vals, color=\"C1\", xlabel=\"Gradient values\")\n",
    "    fig.suptitle(\"Gradient Distribution\", fontsize=14, y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "def plot_activation_distributions(pinn, example_input):\n",
    "    \"\"\"\n",
    "    Capture the intermediate activations of the MambaWavePINN forward pass,\n",
    "    flatten them, and plot with 'plot_dists'.\n",
    "    :param example_input: e.g. your PDE residual data 'res_seq_jnp' \n",
    "                          with shape (N,5,2) or something similar.\n",
    "    \"\"\"\n",
    "    out, intermediates = pinn.get_intermediates(pinn.params, example_input)\n",
    "    flat_acts = flatten_intermediates(intermediates)\n",
    "\n",
    "    activations_dict = {}\n",
    "    idx = 0\n",
    "    for name, arr in flat_acts.items():\n",
    "        # If you only want to see bigger shapes, e.g. arr.ndim > 1\n",
    "        if arr.ndim > 1:\n",
    "            activations_dict[f\"Act_{idx}({name})\"] = arr.ravel()\n",
    "            idx += 1\n",
    "\n",
    "    fig = plot_dists(activations_dict, color=\"C2\", stat=\"density\", xlabel=\"Activation values\")\n",
    "    fig.suptitle(\"Activation Distribution\", fontsize=14, y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wave_solution(\n",
    "    u_2d: np.ndarray,\n",
    "    x_range: tuple = (0,1),\n",
    "    t_range: tuple = (0,1),\n",
    "    title: str = \"Wave Solution\",\n",
    "    cmap: str = \"viridis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a 2D wave solution array in (x,t) space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u_2d : np.ndarray\n",
    "        2D array with shape (n_t, n_x) or (n_x, n_t) \n",
    "        representing solution values u(x,t).\n",
    "\n",
    "    x_range : (float, float)\n",
    "        (min_x, max_x) to map the horizontal axis to [min_x, max_x].\n",
    "\n",
    "    t_range : (float, float)\n",
    "        (min_t, max_t) to map the vertical axis to [min_t, max_t].\n",
    "\n",
    "    title : str\n",
    "        Title for the plot.\n",
    "\n",
    "    cmap : str\n",
    "        Matplotlib colormap name for imshow.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,5))\n",
    "    # If your array is (n_t, n_x), you might want to set `origin=\"lower\"` \n",
    "    # so t=0 is at the bottom. Adjust as needed:\n",
    "    plt.imshow(\n",
    "        u_2d,\n",
    "        extent=[x_range[0], x_range[1], t_range[0], t_range[1]],\n",
    "        aspect='auto',\n",
    "        cmap=cmap,\n",
    "        origin=\"lower\"  # ensures t=0 at the bottom if your array is (t,x)\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"t\")\n",
    "    plt.colorbar(label=\"u(x,t)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) MODEL ARGUMENTS\n",
    "# -------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int = 64\n",
    "    n_layer: int = 2\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 4\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "dense_kernel_init = nn.initializers.xavier_uniform()\n",
    "dense_bias_init = nn.initializers.constant(0.00)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) HELPER MODULES (RMSNorm, MambaBlock, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    d_model: int\n",
    "    eps: float = 1e-5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return x \n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    args: ModelArgs\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Implementation is unchanged from your snippet...\n",
    "        args = self.args\n",
    "        d_model = args.d_model\n",
    "        d_inner = args.d_inner\n",
    "        d_state = args.d_state\n",
    "        dt_rank = args.dt_rank\n",
    "\n",
    "        # Input Projection\n",
    "        x_and_res = nn.Dense(\n",
    "            features=d_inner * 2, use_bias=args.bias,\n",
    "            kernel_init=dense_kernel_init, bias_init=dense_bias_init\n",
    "        )(x)\n",
    "        x_proj, res = jnp.split(x_and_res, 2, axis=-1)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x_conv = x_proj\n",
    "        x_conv = nn.Conv(\n",
    "            features=d_inner,\n",
    "            kernel_size=(args.d_conv,),\n",
    "            feature_group_count=d_inner,\n",
    "            padding='SAME',\n",
    "            use_bias=args.conv_bias\n",
    "        )(x_conv)\n",
    "        x = nn.gelu(x_conv)\n",
    "\n",
    "        # Compute delta, B, C\n",
    "        x_dbl = nn.Dense(\n",
    "            features=dt_rank + 2 * d_state * d_inner,\n",
    "            use_bias=args.bias,\n",
    "            kernel_init=dense_kernel_init,\n",
    "            bias_init=dense_bias_init\n",
    "        )(x)\n",
    "        delta, B_C = jnp.split(x_dbl, [dt_rank], axis=-1)\n",
    "        b, l, _ = delta.shape\n",
    "\n",
    "        B_C = B_C.reshape(b, l, d_inner, 2*d_state)\n",
    "        B, C = jnp.split(B_C, 2, axis=-1) \n",
    "        delta = nn.softplus(\n",
    "            nn.Dense(d_inner, kernel_init=dense_kernel_init, bias_init=dense_bias_init)(delta)\n",
    "        )\n",
    "\n",
    "        # State Space Params\n",
    "        A_log = self.param('A_log', nn.initializers.normal(), (d_inner, d_state))\n",
    "        D = self.param('D', nn.initializers.ones, (d_inner,))\n",
    "        A = -jnp.exp(A_log)\n",
    "\n",
    "        y = self.ssm(x, delta, A, B, C, D)\n",
    "        y = y * nn.gelu(res)\n",
    "        output = nn.Dense(\n",
    "            features=d_model, use_bias=args.bias,\n",
    "            kernel_init=dense_kernel_init, bias_init=dense_bias_init\n",
    "        )(y)\n",
    "        return output\n",
    "\n",
    "    def ssm(self, x, delta, A, B, C, D):\n",
    "        \"\"\"\n",
    "        This is your state-space scan logic (unchanged).\n",
    "        \"\"\"\n",
    "        b, l, d = x.shape\n",
    "        n = A.shape[1]\n",
    "        deltaA = jnp.exp(jnp.einsum('bld,dn->bldn', delta, A))\n",
    "        deltaB_u = jnp.einsum('bld,bldn,bld->bldn', delta, B, x)\n",
    "\n",
    "        x_state = jnp.zeros((b, d, n))\n",
    "\n",
    "        def scan_fn(carry, inputs):\n",
    "            x_prev = carry\n",
    "            deltaA_t, deltaB_u_t = inputs\n",
    "            x_t = deltaA_t * x_prev + deltaB_u_t\n",
    "            return x_t, x_t\n",
    "\n",
    "        deltaA_list = deltaA.transpose(1, 0, 2, 3)   # => (l,b,d,n)\n",
    "        deltaB_u_list = deltaB_u.transpose(1, 0, 2, 3)\n",
    "        _, x_states = jax.lax.scan(scan_fn, x_state, (deltaA_list, deltaB_u_list))\n",
    "        x_states = x_states.transpose(1, 0, 2, 3)  # => (b,l,d,n)\n",
    "\n",
    "        y = jnp.einsum('bldn,bldn->bld', x_states, C)\n",
    "        y = y + x * D[None, None, :]\n",
    "        return y\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    args: ModelArgs\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x_norm = RMSNorm(self.args.d_model)(x)\n",
    "        y = MambaBlock(self.args)(x_norm)\n",
    "        return x + y\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    args: ModelArgs\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, 2)\n",
    "        x = nn.Dense(features=self.args.d_model)(x)\n",
    "        for _ in range(self.args.n_layer):\n",
    "            x = ResidualBlock(self.args)(x)\n",
    "        x = RMSNorm(self.args.d_model)(x)\n",
    "        logits = nn.Dense(features=1, kernel_init=dense_kernel_init, bias_init=dense_bias_init)(x)\n",
    "        return logits  # => (batch_size, seq_len, 1)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) MambaWavePINN (Slightly Extended for Visualization)\n",
    "# -------------------------------------------------------------------------\n",
    "class MambaWavePINN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        key,\n",
    "        args: ModelArgs,\n",
    "        x_seq_res: jnp.ndarray,\n",
    "        t_seq_res: jnp.ndarray,\n",
    "        x_seq_ic: jnp.ndarray,\n",
    "        t_seq_ic: jnp.ndarray,\n",
    "        x_seq_bc: jnp.ndarray,\n",
    "        t_seq_bc: jnp.ndarray,\n",
    "        learning_rate=1e-3,\n",
    "    ):\n",
    "        self.model = Mamba(args)\n",
    "        self.args = args\n",
    "        self.key = key\n",
    "        # Initialize params with a dummy input => shape (1, 5, 2)\n",
    "        dummy_input = jnp.ones((1, 5, 2))\n",
    "        self.params = self.model.init(self.key, dummy_input)\n",
    "\n",
    "        # Store PDE data\n",
    "        self.x_seq_res = x_seq_res\n",
    "        self.t_seq_res = t_seq_res\n",
    "        self.x_seq_ic  = x_seq_ic\n",
    "        self.t_seq_ic  = t_seq_ic\n",
    "        self.x_seq_bc  = x_seq_bc\n",
    "        self.t_seq_bc  = t_seq_bc\n",
    "\n",
    "        self.optimizer = optax.adam(learning_rate)\n",
    "        self.opt_state = self.optimizer.init(self.params)\n",
    "\n",
    "        # Logs\n",
    "        self.loss_log = []\n",
    "        self.loss_ic_log = []\n",
    "        self.loss_bc_log = []\n",
    "        self.loss_res_log = []\n",
    "\n",
    "        self.print_model_summary()\n",
    "\n",
    "    def neural_net(self, params, t_seq, x_seq):\n",
    "        \"\"\" Evaluate Mamba model on (t_seq, x_seq) => shape (batch_size, seq_len, 1). \"\"\"\n",
    "        tx_inputs = jnp.concatenate([t_seq, x_seq], axis=-1)  # => (B,L,2)\n",
    "        return self.model.apply(params, tx_inputs)\n",
    "\n",
    "    def residual_net(self, params, t_seq, x_seq):\n",
    "        \"\"\" PDE residual for wave eqn: u_tt - 4u_xx = 0. \"\"\"\n",
    "        t_flat = t_seq.reshape(-1)\n",
    "        x_flat = x_seq.reshape(-1)\n",
    "\n",
    "        def u_fn(t, x):\n",
    "            inp = jnp.array([[[t, x]]])  # shape (1,1,2)\n",
    "            out = self.model.apply(params, inp)  # shape (1,1,1)\n",
    "            return out[0,0,0]\n",
    "\n",
    "        def u_t_scalar(t, x):\n",
    "            return grad(u_fn, argnums=0)(t, x)\n",
    "\n",
    "        def u_tt_scalar(t, x):\n",
    "            return grad(lambda T: u_t_scalar(T, x))(t)\n",
    "\n",
    "        def u_x_scalar(t, x):\n",
    "            return grad(u_fn, argnums=1)(t, x)\n",
    "\n",
    "        def u_xx_scalar(t, x):\n",
    "            return grad(lambda X: u_x_scalar(t, X))(x)\n",
    "\n",
    "        vmap_u_tt = vmap(u_tt_scalar, in_axes=(0, 0))\n",
    "        vmap_u_xx = vmap(u_xx_scalar, in_axes=(0, 0))\n",
    "        u_tt_vals = vmap_u_tt(t_flat, x_flat)\n",
    "        u_xx_vals = vmap_u_xx(t_flat, x_flat)\n",
    "        r_vals = u_tt_vals - 4.0*u_xx_vals\n",
    "        return r_vals.reshape(t_seq.shape)\n",
    "\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def loss_fn(self, params):\n",
    "        # 1) PDE residual\n",
    "        r_vals = self.residual_net(params, self.t_seq_res, self.x_seq_res)\n",
    "        loss_res = jnp.mean(r_vals**2)\n",
    "\n",
    "        # 2) Initial condition\n",
    "        u_ic_pred = self.neural_net(params, self.t_seq_ic, self.x_seq_ic)  # => (B,L,1)\n",
    "        x_ic_flat = self.x_seq_ic.reshape(-1)\n",
    "        u_ic_true = jnp.sin(jnp.pi*x_ic_flat) - 0.5*jnp.sin(3.0*jnp.pi*x_ic_flat)\n",
    "        u_ic_true = u_ic_true.reshape(u_ic_pred.shape[0], u_ic_pred.shape[1])\n",
    "        loss_ic_1 = jnp.mean((u_ic_pred.squeeze(-1) - u_ic_true)**2)\n",
    "\n",
    "        # Enforce partial u/partial t = 0 at t=0\n",
    "        def scalar_u(t, x):\n",
    "            inp = jnp.array([[[t,x]]])\n",
    "            return self.model.apply(params, inp)[0,0,0]\n",
    "        u_t0 = vmap(lambda xx: grad(scalar_u, argnums=0)(0.0, xx))(x_ic_flat)\n",
    "        loss_ic_2 = jnp.mean(u_t0**2)\n",
    "        loss_ic = loss_ic_1 + loss_ic_2\n",
    "\n",
    "        # 3) Boundary condition => u=0 at x=0 or x=1\n",
    "        u_bc_pred = self.neural_net(params, self.t_seq_bc, self.x_seq_bc)\n",
    "        loss_bc = jnp.mean(u_bc_pred**2)\n",
    "\n",
    "        loss_total = loss_res + loss_ic + loss_bc\n",
    "        return loss_total, (loss_res, loss_ic, loss_bc)\n",
    "\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def update_step(self, params, opt_state):\n",
    "        (loss_val, (loss_res, loss_ic, loss_bc)), grads = jax.value_and_grad(\n",
    "            self.loss_fn, has_aux=True\n",
    "        )(params)\n",
    "        updates, opt_state = self.optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_val, loss_res, loss_ic, loss_bc\n",
    "\n",
    "    def train(self, n_iter=5000, print_every=100):\n",
    "        params = self.params\n",
    "        opt_state = self.opt_state\n",
    "        for i in range(n_iter):\n",
    "            params, opt_state, loss_val, l_res, l_ic, l_bc = self.update_step(params, opt_state)\n",
    "            if i % print_every == 0:\n",
    "                self.loss_log.append(float(loss_val))\n",
    "                self.loss_res_log.append(float(l_res))\n",
    "                self.loss_ic_log.append(float(l_ic))\n",
    "                self.loss_bc_log.append(float(l_bc))\n",
    "                print(f\"[Iter {i}] Loss={loss_val:.3e} Res={l_res:.3e} IC={l_ic:.3e} BC={l_bc:.3e}\")\n",
    "        self.params = params\n",
    "        self.opt_state = opt_state\n",
    "\n",
    "    def predict(self, t_seq, x_seq):\n",
    "        return self.neural_net(self.params, t_seq, x_seq)\n",
    "\n",
    "    def print_model_summary(self):\n",
    "        flat_params = flatten_dict(self.params)\n",
    "        total_params = 0\n",
    "        print(\"Model Summary (MambaWavePINN):\")\n",
    "        for path, param in flat_params.items():\n",
    "            print(f\"{'/'.join(path)}: shape={param.shape}, size={param.size}\")\n",
    "            total_params += param.size\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (A) Capture Activations\n",
    "    # ---------------------------------------------------------------------\n",
    "    def get_intermediates(self, params, example_seq):\n",
    "        \"\"\"\n",
    "        Returns (final_output, intermediates_dict).\n",
    "        The intermediates_dict has structure:\n",
    "          intermediates_dict[\"intermediates\"][module_name] = { ... outputs ... }\n",
    "        \"\"\"\n",
    "        # 'mutable=[\"intermediates\"]' + 'capture_intermediates=True'\n",
    "        # let us see all intermediate activations from the forward pass.\n",
    "        out, info = self.model.apply(\n",
    "            params, example_seq,\n",
    "            capture_intermediates=True,\n",
    "            mutable=[\"intermediates\"]\n",
    "        )\n",
    "        intermediates = info[\"intermediates\"]\n",
    "        return out, intermediates\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (B) Compute Gradients w.r.t. PDE loss\n",
    "    # ---------------------------------------------------------------------\n",
    "    def get_gradients(self, params):\n",
    "        \"\"\"\n",
    "        Return the gradient PyTree (flattened into a list/dict) \n",
    "        for the PDE loss.\n",
    "        \"\"\"\n",
    "        def loss_only(p):\n",
    "            return self.loss_fn(p)[0]  # PDE scalar loss\n",
    "        grads = jax.grad(loss_only)(params)\n",
    "        return grads\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Example Usage & Visualization\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def make_time_sequence(src, num_step=5, step=1e-4):\n",
    "    \"\"\"\n",
    "    For each row of src (which is (x, t)), replicate across seq_len=num_step\n",
    "    and increment t by 'step*i'. => shape (N, num_step, 2).\n",
    "    NOTE: Here we assume src has shape (N,2) => (x_i, t_i).\n",
    "    If your ordering is (t,x), adapt accordingly.\n",
    "    \"\"\"\n",
    "    N = src.shape[0]\n",
    "    seq = np.tile(src[:, None, :], (1, num_step, 1))\n",
    "    for i in range(num_step):\n",
    "        seq[:, i, 1] += step*i  # increment the t-component if it's the 2nd col\n",
    "    return seq\n",
    "\n",
    "def get_data(x_range, y_range, x_num, y_num):\n",
    "    x = np.linspace(x_range[0], x_range[1], x_num)\n",
    "    t = np.linspace(y_range[0], y_range[1], y_num)\n",
    "    x_mesh, t_mesh = np.meshgrid(x, t)\n",
    "    data = np.stack([x_mesh.flatten(), t_mesh.flatten()], axis=-1)  # => (N,2), each row (x,t)\n",
    "\n",
    "    b_left = data[np.isclose(x_mesh.flatten(), x_range[0])]\n",
    "    b_right= data[np.isclose(x_mesh.flatten(), x_range[1])]\n",
    "    b_upper= data[np.isclose(t_mesh.flatten(), y_range[1])]\n",
    "    b_lower= data[np.isclose(t_mesh.flatten(), y_range[0])]\n",
    "\n",
    "    return data, b_left, b_right, b_upper, b_lower\n",
    "\n",
    "def get_data_wave_1d(x_range, t_range, x_num, t_num):\n",
    "    res, b_left, b_right, b_upper, b_lower = get_data(x_range, t_range, x_num, t_num)\n",
    "    return res, b_left, b_right, b_upper, b_lower\n",
    "\n",
    "# exact solution\n",
    "def wave_exact_solution(x, t):\n",
    "    \"\"\"\n",
    "    Example wave eqn solution:\n",
    "      u(x,t) = sin(pi*x)*cos(2*pi*t) + 0.5*sin(3*pi*x)*cos(6*pi*t)\n",
    "    \"\"\"\n",
    "    return np.sin(np.pi*x)*np.cos(2.0*np.pi*t) + 0.5*np.sin(3.0*np.pi*x)*np.cos(6.0*np.pi*t)\n",
    "\n",
    "\n",
    "\n",
    "# 1) Create data and visualize PDE points\n",
    "res, b_left, b_right, b_upper, b_lower = get_data_wave_1d([0,1],[0,1], 31,31)\n",
    "plot_wave_pde_points(res, b_left, b_right, b_lower, b_upper)\n",
    "\n",
    "# 2) Make time sequences\n",
    "# PDE residual data\n",
    "res_seq = make_time_sequence(res, 5, 1e-4)  # shape (N,5,2)\n",
    "res_seq_jnp = jnp.array(res_seq)\n",
    "x_seq_res = res_seq_jnp[:,:,0:1]  # (N,5,1)\n",
    "t_seq_res = res_seq_jnp[:,:,1:2]  # (N,5,1)\n",
    "\n",
    "# Initial condition (t=0 => b_lower)\n",
    "b_lower_seq = make_time_sequence(b_lower, 1, 0.0)\n",
    "b_lower_seq_jnp = jnp.array(b_lower_seq)\n",
    "x_seq_ic = b_lower_seq_jnp[:,:,0:1]\n",
    "t_seq_ic = b_lower_seq_jnp[:,:,1:2]\n",
    "\n",
    "# Boundary condition (x=0 or 1 => b_left + b_right)\n",
    "b_bc = np.concatenate([b_left, b_right], axis=0)\n",
    "b_bc_seq = make_time_sequence(b_bc, 5, 1e-4)\n",
    "b_bc_seq_jnp = jnp.array(b_bc_seq)\n",
    "x_seq_bc = b_bc_seq_jnp[:,:,0:1]\n",
    "t_seq_bc = b_bc_seq_jnp[:,:,1:2]\n",
    "\n",
    "\n",
    "# 2) Initialize the model\n",
    "key = jax.random.PRNGKey(0)\n",
    "args = ModelArgs(d_model=8, n_layer=2, d_state=8, expand=2, dt_rank='auto')\n",
    "pinn = MambaWavePINN(\n",
    "    key=key,\n",
    "    args=args,\n",
    "    x_seq_res=x_seq_res, t_seq_res=t_seq_res,\n",
    "    x_seq_ic=x_seq_ic,   t_seq_ic=t_seq_ic,\n",
    "    x_seq_bc=x_seq_bc,   t_seq_bc=t_seq_bc,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "u_exact = wave_exact_solution(t_seq_res[:,0:1,:], x_seq_res[:,0:1,:]).reshape(31,31)\n",
    "# Suppose u_exact.shape = (31, 31), with t along axis 0 and x along axis 1\n",
    "plot_wave_solution(u_exact, x_range=(0,1), t_range=(0,1), title=\"Exact Wave Solution\")\n",
    "\n",
    "\n",
    "u_pred = pinn.predict(t_seq_res[:,0:1,:], x_seq_res[:,0:1,:]).reshape(31,31)\n",
    "plot_wave_solution(u_pred, x_range=(0,1), t_range=(0,1), title=\"Predicted Wave Solution\")\n",
    "\n",
    "\n",
    "plot_weight_distributions(pinn)\n",
    "plot_gradient_distributions(pinn)\n",
    "plot_activation_distributions(pinn, res_seq_jnp)\n",
    "\n",
    "# Train a bit\n",
    "pinn.train(n_iter=2000, print_every=50)\n",
    "\n",
    "# === Plot in the requested order ===\n",
    "\n",
    "plot_weight_distributions(pinn)\n",
    "plot_gradient_distributions(pinn)\n",
    "plot_activation_distributions(pinn, res_seq_jnp)\n",
    "\n",
    "plot_losses(pinn)\n",
    "u_pred = pinn.predict(t_seq_res[:,0:1,:], x_seq_res[:,0:1,:]).reshape(31,31)\n",
    "plot_wave_solution(u_pred, x_range=(0,1), t_range=(0,1), title=\"Predicted Wave Solution\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
